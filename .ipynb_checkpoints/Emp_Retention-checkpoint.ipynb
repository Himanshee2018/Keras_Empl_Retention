{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To Build a Deep Learning Model to Predict Employee Retention Using Keras and TensorFlow.\n",
    "# The author selected Girls Who Code to receive a donation as part of the Write for DOnations program.\n",
    "#  will predict the probability of an employee leaving a company. Retaining the best employees is an \n",
    "#important factor for most organizations. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"https://raw.githubusercontent.com/mwitiderrick/kerasDO/master/HR_comma_sep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>department</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years department  \\\n",
       "0                   3              0     1                      0      sales   \n",
       "1                   6              0     1                      0      sales   \n",
       "2                   4              0     1                      0      sales   \n",
       "3                   5              0     1                      0      sales   \n",
       "4                   3              0     1                      0      sales   \n",
       "\n",
       "   salary  \n",
       "0     low  \n",
       "1  medium  \n",
       "2  medium  \n",
       "3     low  \n",
       "4     low  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Categorical columns to Numbers. You do this by converting them to dummy variables. Dummy variables are usually ones and zeros that indicate the presence or absence of a categorical feature. In this kind of situation, \n",
    "#you also avoid the dummy variable trap by dropping the first dummy.\n",
    "feats = ['department','salary']\n",
    "df_final = pd.get_dummies(df, columns=feats,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 — Separating Your Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating Training and Test dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38, 0.53, 2.  , ..., 0.  , 1.  , 0.  ],\n",
       "       [0.8 , 0.86, 5.  , ..., 0.  , 0.  , 1.  ],\n",
       "       [0.11, 0.88, 7.  , ..., 0.  , 0.  , 1.  ],\n",
       "       ...,\n",
       "       [0.37, 0.53, 2.  , ..., 0.  , 1.  , 0.  ],\n",
       "       [0.11, 0.96, 6.  , ..., 0.  , 1.  , 0.  ],\n",
       "       [0.37, 0.52, 2.  , ..., 0.  , 1.  , 0.  ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping the \"left Column\". Your deep learning model expects to get the data as arrays. Therefore you use numpy \n",
    "## to convert the data to numpy arrays with the .values attribute.\n",
    "X = df_final.drop(['left'],axis=1).values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_final['left'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping test data as 30 % of total data\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
    "#You'll pass this data to the keras model after converting it into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the dataset. Scaling is important to keep the dataset more efficient.Scaling dataset using \n",
    "# STANDARDSCALER will ensure Mean=0 and  a unit variable or standard deviation 1. Standard Sclaer transforms \n",
    "# the data normally distributed. it will Scale the features within same range. This is imp bcz we are \n",
    "# comparing fetaures having different measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 3 — Transforming the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc= StandardScaler()     # calling and instance of Scnadard Scaler()\n",
    "X_train = sc.fit_transform(X_train)   # fir_tranform to scale the training and test set\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 — Building the Artificial Neural NetworkBUILDING ARTIFICIAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From keras, you'll then import the Sequential module to initialize the artificial neural network. An artificial neural network is a computational model that \n",
    "#is built using inspiration from the workings of the human brain. You'll import the Dense module as well, which will add layers to your deep learning model.\n",
    "\n",
    "#When building a deep learning model you usually specify three layer types:\n",
    "\n",
    "#The input layer is the layer to which you'll pass the features of your dataset. There is no computation that occurs in this layer. It serves to pass features to the hidden layers.\n",
    "#The hidden layers are usually the layers between the input layer and the output layer—and there can be more than one. These layers perform the computations and pass the information to the output layer.\n",
    "#The output layer represents the layer of your neural network that will give you the results after training your model. It is responsible for producing the output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# use Sequential to initialize a linear stack of layers(classfier).\n",
    "from keras.models import Sequential \n",
    "# adding layers to classifier using add function\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier= Sequential()  # Initialising the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Param - No of nodes the NN should have.to find no of nodes is to take average of nodes in input layer & output nodes\n",
    "## Kernel_Initialiser - When you fit your deep learning model the weights will be initialized to numbers close to zero, but not zero. To achieve this you use the uniform distribution initializer. \n",
    "# kernel_initializer is the function that initializes the weights.\n",
    "# Activation Funcion - Deep Leearning modle will learn through this functn. These are LINEAR and NoN-Linear functns.\n",
    "# ReLU generalizes well on data\n",
    "## input_dim - showis no of feartures in your input data\n",
    "classifier.add(Dense(9, kernel_initializer = \"uniform\", activation=\"relu\", input_dim=18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layers which will give us predictions\n",
    "#  The number of output nodes. You expect to get one output: if an employee leaves the company. \n",
    "# Therefore you specify one output node.\n",
    "# For kernel_initializer you use the sigmoid activation function so that you can get the probability \n",
    "# that an employee will leave. In the event that you were dealing with more than two categories, \n",
    "# you would use the softmax activation function, which is a variant of the sigmoid activation function.\n",
    "classifier.add(Dense(1,kernel_initializer = \"uniform\", activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Gradient Descent to minimise the cost function as optimize startegy. Here we will use as Adam\n",
    "classifier.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10499/10499 [==============================] - 1s 104us/step - loss: 0.4114 - acc: 0.7588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3017d6a0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FiTTING your CLASSIfier to the DATAset using .fit() function\n",
    "classifier.fit(X_train,y_train, batch_size=10, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP5 — Running Predictions on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20775685],\n",
       "       [0.20614198],\n",
       "       [0.24818602],\n",
       "       ...,\n",
       "       [0.31802285],\n",
       "       [0.2166252 ],\n",
       "       [0.20651561]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUNNING Prediction on your TEST Dataset\n",
    "y_pred= classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keeping Threshold for 50% of employees leaving companies. Creating predictions using the predict method and \n",
    "# set the threshold for determining if an employee is likely to leav\n",
    "y_pred= (y_pred > 0.5)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP- 6 Checking Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3461,    0],\n",
       "       [1039,    0]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm= confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7 - Making a single prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = classifier.predict(sc.transform(np.array([[0.26,0.7,3.0,238.,6.0,0.,0.,0.,0., 0.,0.,0.,0.,0.,1.,0., 0.,1.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred = (new_pred > 0.5)\n",
    "new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred = (new_pred > 0.6)\n",
    "new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP:8 Improving the Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(9, kernel_initializer = \"uniform\", activation=\"relu\", input_dim=18))\n",
    "    classifier.add(Dense(1,kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
    "    classifier.compile(optimizer=\"adam\", loss =\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier= KerasClassifier(build_fn= make_classifier, batch_size=10, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = cross_val_score(estimator= classifier , X=X_train, y=y_train, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.830464837285982"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean= accuracies.mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = accuracies.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 9: Adding dropout Regularization to fight Over-Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In neural networks, dropout regularization is the technique that fights overfitting by adding a Dropout layer in \n",
    "# your neural network. It has a rate parameter that indicates the number of neurons that will deactivate at each iteration. \n",
    "# The process of deactivating nerurons is usually random. In this case, you specify 0.1 as the rate meaning that 1% of the neurons will deactivate during the training proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(9, kernel_initializer = \"uniform\", activation = \"relu\", input_dim=18))\n",
    "classifier.add(Dropout(rate = 0.1))\n",
    "classifier.add(Dense(1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n",
    "classifier.compile(optimizer= \"adam\",loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 10 : HYPERPATAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def make_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(9, kernel_initializer = \"uniform\", activation = \"relu\", input_dim=18))\n",
    "    classifier.add(Dense(1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n",
    "    classifier.compile(optimizer= optimizer,loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(build_fn = make_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size':[20,35],\n",
    "    'epochs':[2,3],\n",
    "    'optimizer':['adam','rmsprop']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=classifier,\n",
    "                           param_grid=params,\n",
    "                           scoring=\"accuracy\",\n",
    "                           cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5249/5249 [==============================] - 1s 97us/step - loss: 0.5654 - acc: 0.7620\n",
      "Epoch 2/2\n",
      "5249/5249 [==============================] - 0s 44us/step - loss: 0.4120 - acc: 0.7626\n",
      "Epoch 1/2\n",
      "5250/5250 [==============================] - 1s 96us/step - loss: 0.5769 - acc: 0.7537\n",
      "Epoch 2/2\n",
      "5250/5250 [==============================] - 0s 47us/step - loss: 0.4136 - acc: 0.7771\n",
      "Epoch 1/2\n",
      "5249/5249 [==============================] - 0s 87us/step - loss: 0.5714 - acc: 0.7628\n",
      "Epoch 2/2\n",
      "5249/5249 [==============================] - 0s 43us/step - loss: 0.4128 - acc: 0.7914\n",
      "Epoch 1/2\n",
      "5250/5250 [==============================] - 0s 92us/step - loss: 0.5952 - acc: 0.7530\n",
      "Epoch 2/2\n",
      "5250/5250 [==============================] - 0s 42us/step - loss: 0.4412 - acc: 0.7872\n",
      "Epoch 1/3\n",
      "5249/5249 [==============================] - 1s 105us/step - loss: 0.5711 - acc: 0.7596\n",
      "Epoch 2/3\n",
      "5249/5249 [==============================] - 0s 46us/step - loss: 0.4120 - acc: 0.7853\n",
      "Epoch 3/3\n",
      "5249/5249 [==============================] - 0s 49us/step - loss: 0.3592 - acc: 0.8394\n",
      "Epoch 1/3\n",
      "5250/5250 [==============================] - 1s 119us/step - loss: 0.5739 - acc: 0.7549\n",
      "Epoch 2/3\n",
      "5250/5250 [==============================] - 0s 50us/step - loss: 0.4298 - acc: 0.7550\n",
      "Epoch 3/3\n",
      "5250/5250 [==============================] - 0s 50us/step - loss: 0.3819 - acc: 0.7550\n",
      "Epoch 1/3\n",
      "5249/5249 [==============================] - 1s 105us/step - loss: 0.5814 - acc: 0.7617\n",
      "Epoch 2/3\n",
      "5249/5249 [==============================] - 0s 46us/step - loss: 0.4283 - acc: 0.7710\n",
      "Epoch 3/3\n",
      "5249/5249 [==============================] - 0s 44us/step - loss: 0.3707 - acc: 0.8106\n",
      "Epoch 1/3\n",
      "5250/5250 [==============================] - 1s 109us/step - loss: 0.5911 - acc: 0.7528\n",
      "Epoch 2/3\n",
      "5250/5250 [==============================] - 0s 44us/step - loss: 0.4435 - acc: 0.7783\n",
      "Epoch 3/3\n",
      "5250/5250 [==============================] - 0s 42us/step - loss: 0.3759 - acc: 0.8032\n",
      "Epoch 1/2\n",
      "5249/5249 [==============================] - 1s 103us/step - loss: 0.6377 - acc: 0.7628\n",
      "Epoch 2/2\n",
      "5249/5249 [==============================] - 0s 31us/step - loss: 0.4806 - acc: 0.8030\n",
      "Epoch 1/2\n",
      "5250/5250 [==============================] - 1s 110us/step - loss: 0.6333 - acc: 0.7520\n",
      "Epoch 2/2\n",
      "5250/5250 [==============================] - 0s 31us/step - loss: 0.4864 - acc: 0.7760\n",
      "Epoch 1/2\n",
      "5249/5249 [==============================] - 1s 107us/step - loss: 0.6304 - acc: 0.7613\n",
      "Epoch 2/2\n",
      "5249/5249 [==============================] - 0s 31us/step - loss: 0.4838 - acc: 0.7638\n",
      "Epoch 1/2\n",
      "5250/5250 [==============================] - 1s 109us/step - loss: 0.6377 - acc: 0.7531\n",
      "Epoch 2/2\n",
      "5250/5250 [==============================] - 0s 27us/step - loss: 0.5109 - acc: 0.7550\n",
      "Epoch 1/3\n",
      "5249/5249 [==============================] - 1s 138us/step - loss: 0.6290 - acc: 0.7598\n",
      "Epoch 2/3\n",
      "5249/5249 [==============================] - 0s 31us/step - loss: 0.4684 - acc: 0.7775\n",
      "Epoch 3/3\n",
      "5249/5249 [==============================] - 0s 30us/step - loss: 0.3774 - acc: 0.8352\n",
      "Epoch 1/3\n",
      "5250/5250 [==============================] - 1s 126us/step - loss: 0.6346 - acc: 0.7514\n",
      "Epoch 2/3\n",
      "5250/5250 [==============================] - 0s 31us/step - loss: 0.4831 - acc: 0.7554\n",
      "Epoch 3/3\n",
      "5250/5250 [==============================] - 0s 31us/step - loss: 0.4086 - acc: 0.7890\n",
      "Epoch 1/3\n",
      "5249/5249 [==============================] - 1s 122us/step - loss: 0.6301 - acc: 0.7590\n",
      "Epoch 2/3\n",
      "5249/5249 [==============================] - 0s 30us/step - loss: 0.4866 - acc: 0.7636\n",
      "Epoch 3/3\n",
      "5249/5249 [==============================] - 0s 28us/step - loss: 0.4089 - acc: 0.7979\n",
      "Epoch 1/3\n",
      "5250/5250 [==============================] - 1s 135us/step - loss: 0.6377 - acc: 0.7531\n",
      "Epoch 2/3\n",
      "5250/5250 [==============================] - 0s 28us/step - loss: 0.5155 - acc: 0.7829\n",
      "Epoch 3/3\n",
      "5250/5250 [==============================] - 0s 31us/step - loss: 0.4360 - acc: 0.8011\n",
      "Epoch 1/3\n",
      "10499/10499 [==============================] - 1s 83us/step - loss: 0.5580 - acc: 0.7652\n",
      "Epoch 2/3\n",
      "10499/10499 [==============================] - 0s 31us/step - loss: 0.3617 - acc: 0.8459\n",
      "Epoch 3/3\n",
      "10499/10499 [==============================] - 0s 32us/step - loss: 0.2883 - acc: 0.8915\n"
     ]
    }
   ],
   "source": [
    "grid_search = grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 35, 'epochs': 3, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8307457853128869"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
